# ç½‘é¡µå†…å®¹è¯»å–å™¨æ’ä»¶ - å¼€å‘æ–‡æ¡£

## ğŸ“‹ æ’ä»¶æ¦‚è¿°

**æ’ä»¶åç§°**: ç½‘é¡µå†…å®¹è¯»å–å™¨  
**æ¨¡å—åç§°**: web_reader  
**ç‰ˆæœ¬**: 2.0.0  
**ä½œè€…**: liugu  
**æè¿°**: æ™ºèƒ½ç½‘é¡µå†…å®¹æå–å·¥å…·ï¼Œæ”¯æŒå¤šç§ç½‘ç«™ç±»å‹çš„å†…å®¹è¯»å–å’Œè§£æ  

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ’ä»¶ç»“æ„
```
web_reader/
â”œâ”€â”€ __init__.py              # æ’ä»¶å…¥å£æ–‡ä»¶
â”œâ”€â”€ web_reader_plugin.py     # ä¸»è¦æ’ä»¶é€»è¾‘
â”œâ”€â”€ README.md               # ç”¨æˆ·ä½¿ç”¨æ–‡æ¡£
â””â”€â”€ PLUGIN_README.md        # å¼€å‘æ–‡æ¡£
```

### æ ¸å¿ƒç»„ä»¶

#### 1. æ’ä»¶é…ç½®ç±» (WebReaderConfig)
ç»§æ‰¿è‡ª `ConfigBase`ï¼Œæä¾›æ’ä»¶çš„é…ç½®ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒä»¥ä¸‹é…ç½®é¡¹ï¼š
- è¯·æ±‚è¶…æ—¶æ—¶é—´æ§åˆ¶
- å†…å®¹é•¿åº¦é™åˆ¶
- ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
- é“¾æ¥å’Œå›¾ç‰‡æå–å¼€å…³

#### 2. å†…å®¹æå–å™¨ (WebContentExtractor)
è´Ÿè´£è§£æHTMLå†…å®¹å¹¶æå–æœ‰ç”¨ä¿¡æ¯ï¼š
- å…ƒæ•°æ®æå–ï¼ˆæ ‡é¢˜ã€æè¿°ã€å…³é”®è¯ã€ä½œè€…ï¼‰
- ä¸»è¦å†…å®¹æ™ºèƒ½æå–
- é“¾æ¥æå–å’Œæ ¼å¼åŒ–
- å›¾ç‰‡URLæå–

#### 3. æ²™ç®±æ–¹æ³•å®ç°
æä¾›ä¸¤ä¸ªä¸åŒç”¨é€”çš„ç½‘é¡µè¯»å–æ–¹æ³•ï¼š
- **TOOLç±»å‹**: åŸºç¡€ç½‘é¡µè¯»å–ï¼Œæ”¯æŒåŸå§‹HTMLå’Œçº¯æ–‡æœ¬æ¨¡å¼
- **AGENTç±»å‹**: æ™ºèƒ½ç½‘é¡µè§£æï¼Œä½¿ç”¨BeautifulSoupè¿›è¡Œæ·±åº¦åˆ†æ

## ğŸ”§ API æ¥å£è¯¦è§£

### é…ç½®ç±» API

#### WebReaderConfig
```python
@plugin.mount_config()
class WebReaderConfig(ConfigBase):
    DEFAULT_TIMEOUT: int = 30           # è¯·æ±‚è¶…æ—¶æ—¶é—´ (5-300ç§’)
    MAX_CONTENT_LENGTH: int = 15000     # æœ€å¤§å†…å®¹é•¿åº¦ (1000-100000å­—ç¬¦)  
    USER_AGENT: str = "..."             # ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    EXTRACT_LINKS: bool = True          # æ˜¯å¦æå–é“¾æ¥
    EXTRACT_IMAGES: bool = True         # æ˜¯å¦æå–å›¾ç‰‡
```

### æ²™ç®±æ–¹æ³• API

#### 1. TOOLç±»å‹æ–¹æ³• - fetch_webpage
```python
@plugin.mount_sandbox_method(SandboxMethodType.TOOL, "fetch_webpage", "è¯»å–ç½‘é¡µå†…å®¹")
async def fetch_webpage(
    _ctx: AgentCtx, 
    url: str, 
    raw_html: bool = False, 
    timeout: int = None
) -> str
```

**å‚æ•°è¯´æ˜:**
- `_ctx`: Agentä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
- `url`: ç›®æ ‡ç½‘é¡µURLï¼ˆå¿…éœ€å‚æ•°ï¼Œéœ€åŒ…å«http://æˆ–https://ï¼‰
- `raw_html`: æ˜¯å¦è¿”å›åŸå§‹HTMLå†…å®¹ï¼Œé»˜è®¤`False`è¿”å›çº¯æ–‡æœ¬
- `timeout`: è‡ªå®šä¹‰è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼

**è¿”å›å€¼:**
- æˆåŠŸæ—¶è¿”å›æ ¼å¼åŒ–åçš„ç½‘é¡µå†…å®¹æˆ–HTMLæºç 
- å¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ ¼å¼ï¼š`"âŒ é”™è¯¯ï¼šå…·ä½“é”™è¯¯æè¿°"`

**åŠŸèƒ½ç‰¹ç‚¹:**
- åŸºç¡€ç½‘é¡µå†…å®¹æå–
- æ”¯æŒåŸå§‹HTMLå’Œçº¯æ–‡æœ¬ä¸¤ç§è¾“å‡ºæ¨¡å¼
- è‡ªåŠ¨å¤„ç†ç½‘é¡µç¼–ç æ£€æµ‹å’Œè½¬æ¢
- å†…å®¹é•¿åº¦é™åˆ¶å’Œæ™ºèƒ½æˆªæ–­
- å®Œå–„çš„URLæ ¼å¼éªŒè¯

#### 2. AGENTç±»å‹æ–¹æ³• - fetch_webpage  
```python
@plugin.mount_sandbox_method(SandboxMethodType.AGENT, "fetch_webpage", "è¯»å–å¹¶è§£æç½‘é¡µå†…å®¹")
async def fetch_webpage(
    _ctx: AgentCtx, 
    url: str, 
    timeout: int = None
) -> str
```

**å‚æ•°è¯´æ˜:**
- `_ctx`: Agentä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
- `url`: ç›®æ ‡ç½‘é¡µURLï¼ˆå¿…éœ€å‚æ•°ï¼‰
- `timeout`: è‡ªå®šä¹‰è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼

**è¿”å›å€¼:**
- æˆåŠŸæ—¶è¿”å›æ™ºèƒ½è§£æåçš„å®Œæ•´ç½‘é¡µä¿¡æ¯ï¼ŒåŒ…å«æ ¼å¼åŒ–è¾“å‡º
- å¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²

**åŠŸèƒ½ç‰¹ç‚¹:**
- æ™ºèƒ½HTMLè§£æï¼ˆä½¿ç”¨BeautifulSoup4ï¼‰
- å…ƒæ•°æ®æå–ï¼ˆæ ‡é¢˜ã€æè¿°ã€å…³é”®è¯ã€ä½œè€…ä¿¡æ¯ï¼‰
- ä¸»è¦å†…å®¹æ™ºèƒ½æå–ï¼Œè¿‡æ»¤å¯¼èˆªå’Œå¹¿å‘Š
- é“¾æ¥æå–ï¼ˆæœ€å¤š10ä¸ªä¸»è¦é“¾æ¥ï¼‰
- å›¾ç‰‡URLæå–ï¼ˆæœ€å¤š5ä¸ªå›¾ç‰‡ï¼‰
- ç»“æ„åŒ–æ ¼å¼åŒ–è¾“å‡º

### å·¥å…·ç±» API

#### WebContentExtractor
å†…å®¹æå–å™¨æ ¸å¿ƒç±»ï¼Œæä¾›ä»¥ä¸‹é™æ€æ–¹æ³•ï¼š

##### clean_text(text: str) -> str
æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·ã€‚

##### extract_metadata(soup) -> Dict[str, str]
æå–ç½‘é¡µå…ƒæ•°æ®ï¼Œè¿”å›å­—å…¸åŒ…å«ï¼š
- `title`: ç½‘é¡µæ ‡é¢˜ï¼ˆä»<title>æ ‡ç­¾æå–ï¼‰
- `description`: é¡µé¢æè¿°ï¼ˆä»meta descriptionæå–ï¼‰
- `keywords`: å…³é”®è¯ï¼ˆä»meta keywordsæå–ï¼‰
- `author`: ä½œè€…ä¿¡æ¯ï¼ˆä»meta authoræå–ï¼‰

##### extract_main_content(soup) -> str
æ™ºèƒ½æå–ç½‘é¡µä¸»è¦å†…å®¹ï¼Œç®—æ³•ç‰¹ç‚¹ï¼š
- è¯†åˆ«å¹¶ç§»é™¤å¯¼èˆªæ ã€å¹¿å‘Šã€é¡µè„šç­‰æ— å…³å†…å®¹
- ä¿ç•™æ–‡ç« æ­£æ–‡ã€ä¸»è¦æ–‡æœ¬å†…å®¹
- æ¸…ç†HTMLæ ‡ç­¾ä½†ä¿ç•™æ–‡æœ¬ç»“æ„

##### extract_links(soup, base_url: str, limit: int = 10) -> List[Dict[str, str]]
æå–ç½‘é¡µä¸­çš„é“¾æ¥ï¼Œè¿”å›æ ¼å¼ï¼š
```python
[
    {
        "text": "é“¾æ¥æ˜¾ç¤ºæ–‡æœ¬",
        "url": "ç»å¯¹URLåœ°å€",
        "title": "é“¾æ¥titleå±æ€§"
    }
]
```
- è‡ªåŠ¨å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
- è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥å’Œé”šç‚¹é“¾æ¥
- é™åˆ¶è¿”å›æ•°é‡é¿å…è¿‡å¤š

##### extract_images(soup, base_url: str, limit: int = 5) -> List[str]
æå–ç½‘é¡µä¸­çš„å›¾ç‰‡URLï¼š
- è‡ªåŠ¨è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
- è¿‡æ»¤æ‰æ— æ•ˆå›¾ç‰‡é“¾æ¥
- è¿”å›å›¾ç‰‡URLåˆ—è¡¨

## âš™ï¸ é…ç½®è¯¦è§£

### é…ç½®é¡¹è®¾è®¡

| é…ç½®é¡¹ | æ•°æ®ç±»å‹ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | è®¾è®¡è¯´æ˜ |
|--------|----------|--------|----------|----------|
| DEFAULT_TIMEOUT | int | 30 | 5-300 | å¹³è¡¡ç”¨æˆ·ä½“éªŒå’Œç­‰å¾…æ—¶é—´ï¼Œé˜²æ­¢æ— é™ç­‰å¾… |
| MAX_CONTENT_LENGTH | int | 15000 | 1000-100000 | é˜²æ­¢å†…å­˜æº¢å‡ºï¼ŒåŒæ—¶ä¿è¯è¶³å¤Ÿçš„å†…å®¹æå– |
| USER_AGENT | str | Chrome 120 | - | æ¨¡æ‹Ÿç°ä»£æµè§ˆå™¨ï¼Œé¿å…è¢«ç½‘ç«™å±è”½ |
| EXTRACT_LINKS | bool | true | - | é»˜è®¤å¼€å¯ï¼Œæä¾›å®Œæ•´çš„ç½‘é¡µä¿¡æ¯ |
| EXTRACT_IMAGES | bool | true | - | é»˜è®¤å¼€å¯ï¼Œæ”¯æŒå¤šåª’ä½“å†…å®¹åˆ†æ |

### é…ç½®ä½¿ç”¨æ–¹å¼
```python
# è·å–é…ç½®å®ä¾‹
config = plugin.get_config(WebReaderConfig)

# ä½¿ç”¨é…ç½®å€¼ï¼ˆæ”¯æŒè‡ªå®šä¹‰æˆ–é»˜è®¤å€¼ï¼‰
timeout = timeout or config.DEFAULT_TIMEOUT
max_length = config.MAX_CONTENT_LENGTH
user_agent = config.USER_AGENT
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†è®¾è®¡

### é”™è¯¯åˆ†ç±»å’Œå¤„ç†ç­–ç•¥

#### 1. å‚æ•°éªŒè¯é”™è¯¯
```python
# URLæ ¼å¼éªŒè¯
if not url or not isinstance(url, str):
    return "âŒ é”™è¯¯ï¼šURLä¸èƒ½ä¸ºç©º"

# URLç»“æ„éªŒè¯  
try:
    parsed = urlparse(url)
    if not all([parsed.scheme, parsed.netloc]):
        return f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„URL '{url}'ï¼Œéœ€è¦åŒ…å« http:// æˆ– https://"
except Exception as e:
    return f"âŒ é”™è¯¯ï¼šURLè§£æå¤±è´¥ - {str(e)}"
```

#### 2. ç½‘ç»œè¯·æ±‚é”™è¯¯
```python
except requests.exceptions.Timeout:
    return f"âŒ é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œç›®æ ‡ç½‘ç«™å“åº”è¿‡æ…¢"
except requests.exceptions.ConnectionError as e:
    return f"âŒ é”™è¯¯ï¼šè¿æ¥å¤±è´¥ - {str(e)}ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
except requests.exceptions.HTTPError as e:
    status_code = e.response.status_code if hasattr(e, 'response') else 'æœªçŸ¥'
    return f"âŒ é”™è¯¯ï¼šHTTP {status_code} - æœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç "
```

#### 3. å†…å®¹å¤„ç†é”™è¯¯
```python
except Exception as e:
    return f"âŒ é”™è¯¯ï¼šå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ - {type(e).__name__}: {str(e)}"
```

### é”™è¯¯ä¿¡æ¯è®¾è®¡åŸåˆ™
- ç»Ÿä¸€çš„é”™è¯¯å‰ç¼€ï¼š`"âŒ é”™è¯¯ï¼š"`
- å…·ä½“çš„é”™è¯¯æè¿°å’Œå¯èƒ½åŸå› 
- å‹å¥½çš„ç”¨æˆ·æç¤ºå’Œè§£å†³å»ºè®®
- åŒ…å«å…³é”®å‚æ•°ä¿¡æ¯ï¼ˆå¦‚è¶…æ—¶æ—¶é—´ã€çŠ¶æ€ç ç­‰ï¼‰

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### å†…å­˜ç®¡ç†
- **å†…å®¹é•¿åº¦é™åˆ¶**: é€šè¿‡ `MAX_CONTENT_LENGTH` é…ç½®é˜²æ­¢å†…å­˜æº¢å‡º
- **æµå¼å¤„ç†**: å¤§æ–‡ä»¶é‡‡ç”¨åˆ†æ®µè¯»å–å’Œå¤„ç†ç­–ç•¥
- **åŠæ—¶æ¸…ç†**: å¤„ç†å®ŒæˆååŠæ—¶æ¸…ç†ä¸´æ—¶å˜é‡å’Œå¤§å‹å¯¹è±¡
- **å­—ç¬¦ä¸²ä¼˜åŒ–**: ä½¿ç”¨é«˜æ•ˆçš„å­—ç¬¦ä¸²å¤„ç†æ–¹æ³•ï¼Œé¿å…é‡å¤æ‹¼æ¥

### ç½‘ç»œä¼˜åŒ–
- **è¿æ¥æ± å¤ç”¨**: ä½¿ç”¨requestså†…ç½®çš„è¿æ¥æ± æœºåˆ¶
- **è¶…æ—¶æ§åˆ¶**: å¤šå±‚æ¬¡çš„è¶…æ—¶è®¾ç½®ï¼ˆè¿æ¥è¶…æ—¶ã€è¯»å–è¶…æ—¶ï¼‰
- **é‡å®šå‘å¤„ç†**: è‡ªåŠ¨å¤„ç†HTTPé‡å®šå‘ï¼Œæœ€å¤šæ”¯æŒ30æ¬¡è·³è½¬
- **è¯·æ±‚å¤´ä¼˜åŒ–**: åˆç†çš„User-Agentå’Œè¯·æ±‚å¤´è®¾ç½®

### è§£æä¼˜åŒ–
- **é€‰æ‹©æ€§è§£æ**: åªæå–éœ€è¦çš„HTMLå…ƒç´ ï¼Œé¿å…å…¨æ–‡æ¡£æ‰«æ
- **ç¼“å­˜æœºåˆ¶**: é‡å¤ä½¿ç”¨çš„é€‰æ‹©å™¨ç»“æœè¿›è¡Œç¼“å­˜
- **ç¼–ç æ£€æµ‹**: å¿«é€Ÿæ£€æµ‹ç½‘é¡µç¼–ç ï¼Œé¿å…ç¼–ç é”™è¯¯å¯¼è‡´çš„é‡å¤è¯·æ±‚
- **ç®—æ³•ä¼˜åŒ–**: ä½¿ç”¨é«˜æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼å’Œé€‰æ‹©å™¨

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### æ—¥å¿—è®°å½•ç‚¹
```python
# å…³é”®æ­¥éª¤è®°å½•è°ƒè¯•ä¿¡æ¯
print(f"æ­£åœ¨è¯·æ±‚URL: {url}")
print(f"æ£€æµ‹åˆ°ç¼–ç : {encoding}")
print(f"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
```

### æ€§èƒ½ç›‘æ§æŒ‡æ ‡
- è¯·æ±‚å“åº”æ—¶é—´ç»Ÿè®¡
- å†…å®¹æå–è€—æ—¶åˆ†æ  
- å†…å­˜ä½¿ç”¨æƒ…å†µç›‘æ§
- é”™è¯¯å‘ç”Ÿé¢‘ç‡ç»Ÿè®¡

### è°ƒè¯•å»ºè®®
```python
# å¼€å‘è°ƒè¯•æ—¶å¯å¼€å¯è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ€§èƒ½åˆ†æ
import time
start_time = time.time()
# ... æ‰§è¡Œæ“ä½œ
print(f"è€—æ—¶: {time.time() - start_time:.2f}ç§’")
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•
```python
# é…ç½®æµ‹è¯•
def test_config_validation():
    config = WebReaderConfig()
    assert 5 <= config.DEFAULT_TIMEOUT <= 300
    assert 1000 <= config.MAX_CONTENT_LENGTH <= 100000

# URLéªŒè¯æµ‹è¯•
def test_url_validation():
    assert validate_url("https://example.com") == True
    assert validate_url("invalid-url") == False

# å†…å®¹æå–æµ‹è¯•
def test_content_extraction():
    html = "<html><title>æµ‹è¯•</title><body>å†…å®¹</body></html>"
    result = extract_main_content(html)
    assert "æµ‹è¯•" in result
    assert "å†…å®¹" in result
```

### é›†æˆæµ‹è¯•
```python
# çœŸå®ç½‘ç«™æµ‹è¯•
urls = [
    "https://httpbin.org/html",           # åŸºç¡€HTML
    "https://httpbin.org/redirect/3",     # é‡å®šå‘æµ‹è¯•
    "https://httpbin.org/status/404",     # é”™è¯¯çŠ¶æ€ç 
    "https://httpbin.org/delay/5",        # è¶…æ—¶æµ‹è¯•
]

for url in urls:
    result = await fetch_webpage(url)
    assert result is not None
    print(f"{url}: æˆåŠŸ")
```

### æ€§èƒ½æµ‹è¯•
```python
# å¤§æ–‡ä»¶å¤„ç†æµ‹è¯•
large_url = "https://httpbin.org/encoding/utf8"
result = await fetch_webpage(large_url)
assert len(result) <= config.MAX_CONTENT_LENGTH

# å¹¶å‘æµ‹è¯•ï¼ˆå¦‚éœ€è¦ï¼‰
import asyncio
urls = ["https://example.com"] * 10
tasks = [fetch_webpage(url) for url in urls]
results = await asyncio.gather(*tasks)
```

## ğŸ“š æœ€ä½³å®è·µ

### ä½¿ç”¨å»ºè®®
1. **åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´**: æ ¹æ®ç½‘ç»œç¯å¢ƒå’Œç›®æ ‡ç½‘ç«™å“åº”é€Ÿåº¦è°ƒæ•´
2. **é€‚å½“é™åˆ¶å†…å®¹é•¿åº¦**: é¿å…å¤„ç†è¿‡å¤§çš„ç½‘é¡µå†…å®¹ï¼Œæé«˜å“åº”é€Ÿåº¦
3. **é”™è¯¯é‡è¯•æœºåˆ¶**: å¯¹é‡è¦è¯·æ±‚å®ç°æŒ‡æ•°é€€é¿é‡è¯•é€»è¾‘
4. **ç¼“å­˜ç­–ç•¥**: å¯¹é¢‘ç¹è®¿é—®çš„ç½‘é¡µè€ƒè™‘æœ¬åœ°ç¼“å­˜æœºåˆ¶

### å®‰å…¨å»ºè®®
1. **URLç™½åå•**: å¯¹å¯è®¿é—®çš„åŸŸåè¿›è¡Œé™åˆ¶ï¼Œé˜²æ­¢SSRFæ”»å‡»
2. **å†…å®¹è¿‡æ»¤**: å¯¹è¿”å›å†…å®¹è¿›è¡Œå®‰å…¨è¿‡æ»¤ï¼Œé˜²æ­¢XSS
3. **é¢‘ç‡é™åˆ¶**: é¿å…å¯¹åŒä¸€ç½‘ç«™è¿‡äºé¢‘ç¹çš„è¯·æ±‚
4. **ç”¨æˆ·ä»£ç†è½®æ¢**: å®šæœŸæ›´æ¢User-Agenté¿å…è¢«å°ç¦

### æ‰©å±•å¼€å‘æŒ‡å—

#### æ·»åŠ æ–°çš„æå–åŠŸèƒ½
```python
class WebContentExtractor:
    @staticmethod
    def extract_tables(soup) -> List[Dict]:
        """æå–ç½‘é¡µä¸­çš„è¡¨æ ¼æ•°æ®"""
        tables = []
        for table in soup.find_all('table'):
            # å®ç°è¡¨æ ¼æå–é€»è¾‘
            tables.append(table_data)
        return tables
```

#### è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼
```python
def format_custom_output(metadata, content, links, images):
    """è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼"""
    return {
        "meta": metadata,
        "content": content,
        "links": links,
        "images": images,
        "timestamp": datetime.now().isoformat()
    }
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### ä¾èµ–åº“æ–‡æ¡£
- **requests**: https://docs.python-requests.org/
- **BeautifulSoup4**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **nekro-agent**: å‚è€ƒå®˜æ–¹æ’ä»¶å¼€å‘æ–‡æ¡£

### å¸¸è§é—®é¢˜è§£å†³
1. **ç¼–ç é—®é¢˜**: æ£€æŸ¥ç½‘é¡µcharsetå£°æ˜å’Œå®é™…ç¼–ç æ˜¯å¦ä¸€è‡´
2. **è¶…æ—¶é—®é¢˜**: è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§
3. **å†…å®¹æˆªæ–­**: å¢åŠ MAX_CONTENT_LENGTHé…ç½®å€¼
4. **æå–å¤±è´¥**: æ£€æŸ¥ç½‘é¡µç»“æ„æ˜¯å¦å‘ç”Ÿé‡å¤§å˜åŒ–

### æ€§èƒ½è°ƒä¼˜
- ä½¿ç”¨è¿æ¥æ± å¤ç”¨HTTPè¿æ¥
- å®ç°æ™ºèƒ½ç¼“å­˜æœºåˆ¶
- ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼å’Œé€‰æ‹©å™¨æ€§èƒ½
- è€ƒè™‘ä½¿ç”¨å¼‚æ­¥IOæé«˜å¹¶å‘èƒ½åŠ›

---

<div align="center">
  <sub>æ™ºèƒ½ç½‘é¡µå†…å®¹æå–ï¼Œè®©æ•°æ®è·å–æ›´ç®€å• ğŸŒ</sub>
</div>