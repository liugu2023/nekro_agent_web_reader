from nekro_agent.api.plugin import NekroPlugin, dynamic_import_pkg, SandboxMethodType, ConfigBase
from nekro_agent.api.schemas import AgentCtx
from pydantic import Field
import re
from urllib.parse import urlparse, urljoin
from typing import Dict, List, Optional

# åˆ›å»ºæ’ä»¶å®ä¾‹
plugin = NekroPlugin(
    name="å¢å¼ºç‰ˆç½‘é¡µå†…å®¹è¯»å–å™¨",
    module_name="web_reader",
    description="æ”¯æŒå¤šç§ç½‘ç«™ç±»å‹çš„æ™ºèƒ½ç½‘é¡µå†…å®¹æå–å·¥å…·",
    version="2.0.0",
    author="liugu",
    url="none"
)

# åŠ¨æ€å¯¼å…¥å¤–éƒ¨ä¾èµ–
requests = dynamic_import_pkg("requests>=2.25.0,<3.0.0")
bs4 = dynamic_import_pkg("beautifulsoup4>=4.9.0,<5.0.0", import_name="bs4")

@plugin.mount_config()
class WebReaderConfig(ConfigBase):
    """ç½‘é¡µå†…å®¹è¯»å–å™¨é…ç½®"""
    
    DEFAULT_TIMEOUT: int = Field(
        default=30,
        title="é»˜è®¤è¯·æ±‚è¶…æ—¶æ—¶é—´",
        description="HTTPè¯·æ±‚çš„é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰",
        ge=5,
        le=300,
    )
    
    MAX_CONTENT_LENGTH: int = Field(
        default=15000,
        title="æœ€å¤§å†…å®¹é•¿åº¦",
        description="è¿”å›å†…å®¹çš„æœ€å¤§å­—ç¬¦æ•°",
        ge=1000,
        le=100000,
    )
    
    USER_AGENT: str = Field(
        default="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        title="ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²",
        description="HTTPè¯·æ±‚æ—¶ä½¿ç”¨çš„User-Agentå¤´",
    )
    
    EXTRACT_LINKS: bool = Field(
        default=True,
        title="æå–é“¾æ¥",
        description="æ˜¯å¦æå–é¡µé¢ä¸­çš„ä¸»è¦é“¾æ¥",
    )
    
    EXTRACT_IMAGES: bool = Field(
        default=True,
        title="æå–å›¾ç‰‡",
        description="æ˜¯å¦æå–é¡µé¢ä¸­çš„å›¾ç‰‡URL",
    )

config: WebReaderConfig = plugin.get_config(WebReaderConfig)


class WebContentExtractor:
    """ç½‘é¡µå†…å®¹æå–å™¨"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        # æ›¿æ¢å¤šä¸ªç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        return text
    
    @staticmethod
    def extract_metadata(soup) -> Dict[str, str]:
        """æå–ç½‘é¡µå…ƒæ•°æ®"""
        metadata = {}
        
        # æå–æ ‡é¢˜
        title_tag = soup.find('title')
        metadata['title'] = title_tag.get_text().strip() if title_tag else "æ— æ ‡é¢˜"
        
        # æå–æè¿°
        desc_tag = soup.find('meta', attrs={'name': 'description'}) or \
                   soup.find('meta', attrs={'property': 'og:description'})
        metadata['description'] = desc_tag.get('content', '').strip() if desc_tag else ""
        
        # æå–å…³é”®è¯
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        metadata['keywords'] = keywords_tag.get('content', '').strip() if keywords_tag else ""
        
        # æå–ä½œè€…
        author_tag = soup.find('meta', attrs={'name': 'author'}) or \
                     soup.find('meta', attrs={'property': 'article:author'})
        metadata['author'] = author_tag.get('content', '').strip() if author_tag else ""
        
        return metadata
    
    @staticmethod
    def extract_main_content(soup) -> str:
        """æ™ºèƒ½æå–ä¸»è¦å†…å®¹"""
        # ç§»é™¤æ— ç”¨æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']):
            tag.decompose()
        
        # ä¼˜å…ˆæŸ¥æ‰¾æ–‡ç« ä¸»ä½“
        main_content = None
        
        # å¸¸è§çš„æ–‡ç« å®¹å™¨
        content_selectors = [
            'article',
            '[role="main"]',
            'main',
            '.article-content',
            '.post-content',
            '.entry-content',
            '#content',
            '.content',
        ]
        
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ body
        if not main_content:
            main_content = soup.find('body')
        
        if not main_content:
            return ""
        
        # æå–æ–‡æœ¬
        text = main_content.get_text(separator='\n', strip=True)
        return WebContentExtractor.clean_text(text)
    
    @staticmethod
    def extract_links(soup, base_url: str, limit: int = 10) -> List[Dict[str, str]]:
        """æå–é‡è¦é“¾æ¥"""
        links = []
        seen_urls = set()
        
        for a_tag in soup.find_all('a', href=True):
            if len(links) >= limit:
                break
            
            href = a_tag.get('href', '').strip()
            text = a_tag.get_text().strip()
            
            # è·³è¿‡ç©ºé“¾æ¥ã€é”šç‚¹ã€JavaScript
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue
            
            # è½¬æ¢ä¸ºç»å¯¹URL
            absolute_url = urljoin(base_url, href)
            
            # å»é‡
            if absolute_url in seen_urls:
                continue
            seen_urls.add(absolute_url)
            
            # åªä¿ç•™ http/https é“¾æ¥
            if absolute_url.startswith(('http://', 'https://')):
                links.append({
                    'text': text[:50] if text else 'æ— æ–‡æœ¬',
                    'url': absolute_url
                })
        
        return links
    
    @staticmethod
    def extract_images(soup, base_url: str, limit: int = 10) -> List[str]:
        """æå–å›¾ç‰‡URL"""
        images = []
        seen_urls = set()
        
        for img_tag in soup.find_all('img'):
            if len(images) >= limit:
                break
            
            # å°è¯•å¤šä¸ªå±æ€§
            src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-lazy-src')
            
            if not src:
                continue
            
            # è½¬æ¢ä¸ºç»å¯¹URL
            absolute_url = urljoin(base_url, src.strip())
            
            # å»é‡å’Œè¿‡æ»¤
            if absolute_url in seen_urls or not absolute_url.startswith(('http://', 'https://')):
                continue
            
            # è¿‡æ»¤æ‰å°å›¾æ ‡å’Œåƒç´ å›¾
            if any(x in absolute_url.lower() for x in ['icon', 'logo', '1x1', 'pixel']):
                continue
            
            seen_urls.add(absolute_url)
            images.append(absolute_url)
        
        return images


@plugin.mount_sandbox_method(SandboxMethodType.AGENT, "fetch_webpage", "è¯»å–å¹¶è§£æç½‘é¡µå†…å®¹")
async def fetch_webpage(_ctx: AgentCtx, url: str, timeout: int = None) -> str:
    """è¯»å–å¹¶æ™ºèƒ½è§£æç½‘é¡µå†…å®¹
    
    Args:
        url: è¦è¯»å–çš„ç½‘å€
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„ç½‘é¡µå†…å®¹
    """
    try:
        # æ£€æŸ¥ä¾èµ–
        if not requests:
            return "âŒ é”™è¯¯ï¼šrequestsåŒ…æœªå®‰è£…"
        
        if not bs4:
            return "âš ï¸ è­¦å‘Šï¼šBeautifulSoupæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼"
        
        # å‚æ•°éªŒè¯
        if not url or not isinstance(url, str):
            return "âŒ é”™è¯¯ï¼šURLä¸èƒ½ä¸ºç©º"
        
        # URLæ ¼å¼éªŒè¯
        try:
            parsed = urlparse(url)
            if not all([parsed.scheme, parsed.netloc]):
                return f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„URL '{url}'ï¼Œéœ€è¦åŒ…å« http:// æˆ– https://"
        except Exception as e:
            return f"âŒ é”™è¯¯ï¼šURLè§£æå¤±è´¥ - {str(e)}"
        
        # é…ç½®
        timeout = timeout or config.DEFAULT_TIMEOUT
        max_length = config.MAX_CONTENT_LENGTH
        headers = {'User-Agent': config.USER_AGENT}
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)
        response.raise_for_status()
        
        # å¤„ç†ç¼–ç 
        if response.encoding:
            response.encoding = response.encoding
        else:
            response.encoding = response.apparent_encoding or 'utf-8'
        
        content = response.text
        
        # ä½¿ç”¨ BeautifulSoup è§£æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if bs4:
            BeautifulSoup = bs4.BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')
            extractor = WebContentExtractor()
            
            # æå–å…ƒæ•°æ®
            metadata = extractor.extract_metadata(soup)
            
            # æå–ä¸»è¦å†…å®¹
            main_text = extractor.extract_main_content(soup)
            
            # æå–é“¾æ¥
            links = []
            if config.EXTRACT_LINKS:
                links = extractor.extract_links(soup, url, limit=10)
            
            # æå–å›¾ç‰‡
            images = []
            if config.EXTRACT_IMAGES:
                images = extractor.extract_images(soup, url, limit=5)
            
            # æ ¼å¼åŒ–è¾“å‡º
            output_parts = [
                "=" * 60,
                "ğŸ“„ ç½‘é¡µä¿¡æ¯",
                "=" * 60,
                f"ğŸ”— URL: {url}",
                f"ğŸ“Œ æ ‡é¢˜: {metadata['title']}",
                f"âœ… çŠ¶æ€ç : {response.status_code}",
                f"ğŸŒ ç¼–ç : {response.encoding}",
            ]
            
            if metadata.get('description'):
                output_parts.append(f"ğŸ“ æè¿°: {metadata['description']}")
            
            if metadata.get('author'):
                output_parts.append(f"âœï¸ ä½œè€…: {metadata['author']}")
            
            if metadata.get('keywords'):
                output_parts.append(f"ğŸ·ï¸ å…³é”®è¯: {metadata['keywords']}")
            
            # ä¸»è¦å†…å®¹
            output_parts.extend([
                "",
                "=" * 60,
                "ğŸ“– ä¸»è¦å†…å®¹",
                "=" * 60,
            ])
            
            if main_text:
                preview = main_text[:max_length]
                if len(main_text) > max_length:
                    preview += "\n\n... (å†…å®¹å·²æˆªæ–­)"
                output_parts.append(preview)
            else:
                output_parts.append("ï¼ˆæœªæ‰¾åˆ°ä¸»è¦å†…å®¹ï¼‰")
            
            output_parts.append(f"\nğŸ“Š æ€»å­—æ•°: {len(main_text)}")
            
            # é“¾æ¥
            if links:
                output_parts.extend([
                    "",
                    "=" * 60,
                    f"ğŸ”— é‡è¦é“¾æ¥ (å…±{len(links)}ä¸ª)",
                    "=" * 60,
                ])
                for i, link in enumerate(links, 1):
                    output_parts.append(f"{i}. {link['text']}")
                    output_parts.append(f"   {link['url']}")
            
            # å›¾ç‰‡
            if images:
                output_parts.extend([
                    "",
                    "=" * 60,
                    f"ğŸ–¼ï¸ å›¾ç‰‡èµ„æº (å…±{len(images)}ä¸ª)",
                    "=" * 60,
                ])
                for i, img_url in enumerate(images, 1):
                    output_parts.append(f"{i}. {img_url}")
            
            return "\n".join(output_parts)
        
        else:
            # ç®€åŒ–æ¨¡å¼ï¼ˆæ—  BeautifulSoupï¼‰
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "æ— æ ‡é¢˜"
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            clean = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
            clean = re.sub(r'<style[^>]*>.*?</style>', '', clean, flags=re.DOTALL | re.IGNORECASE)
            
            # ç§»é™¤HTMLæ ‡ç­¾
            text = re.sub(r'<[^>]+>', '', clean)
            text = re.sub(r'\s+', ' ', text).strip()
            
            preview = text[:max_length]
            if len(text) > max_length:
                preview += "\n\n... (å†…å®¹å·²æˆªæ–­)"
            
            return f"""{"=" * 60}
ğŸ“„ ç½‘é¡µä¿¡æ¯ (ç®€åŒ–æ¨¡å¼)
{"=" * 60}
ğŸ”— URL: {url}
ğŸ“Œ æ ‡é¢˜: {title}
âœ… çŠ¶æ€ç : {response.status_code}
ğŸŒ ç¼–ç : {response.encoding}

{"=" * 60}
ğŸ“– å†…å®¹
{"=" * 60}
{preview}

ğŸ“Š æ€»å­—æ•°: {len(text)}

âš ï¸ æç¤º: å®‰è£… beautifulsoup4 ä»¥è·å¾—æ›´å¥½çš„è§£ææ•ˆæœ"""
        
    except requests.exceptions.Timeout:
        return f"âŒ é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œç›®æ ‡ç½‘ç«™å“åº”è¿‡æ…¢"
    except requests.exceptions.ConnectionError:
        return "âŒ é”™è¯¯ï¼šè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®"
    except requests.exceptions.HTTPError as e:
        status = e.response.status_code if hasattr(e, 'response') else 'æœªçŸ¥'
        return f"âŒ é”™è¯¯ï¼šHTTP {status} - æœåŠ¡å™¨è¿”å›é”™è¯¯"
    except requests.exceptions.RequestException as e:
        return f"âŒ é”™è¯¯ï¼šè¯·æ±‚å¼‚å¸¸ - {str(e)}"
    except Exception as e:
        return f"âŒ é”™è¯¯ï¼š{type(e).__name__}: {str(e)}"